\documentclass[12pt,xcolor=svgnames]{beamer}
\usepackage{dsfont,natbib,setspace}
\usepackage{tabularx}
\usepackage{accents}
\mode<presentation>

% replaces beamer foot with simple page number
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}{
  \vspace{-1.5cm}
  \raisebox{10pt}{\makebox[\paperwidth]{\hfill\makebox[20pt]{\color{gray}\scriptsize\insertpagenumber}}}}

% colors
\newcommand{\bk}{\color{black}}
\newcommand{\rd}{\color{red}}
\newcommand{\fg}{\color{ForestGreen}}
\newcommand{\mar}{\color{Maroon}}
\newcommand{\org}{\color{Orange}}
\newcommand{\bl}{\color{blue}}
\newcommand{\gr}{\color{gray}}
\newcommand{\theme}{\color{FireBrick}}
\newcommand{\fb}{\color{FireBrick}}
\newcommand{\hlf}{\setstretch{1}}

% common math markups
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\bm}[1]{\mbox{\boldmath $#1$}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\ds}[1]{\mathds{#1}}

% spacing and style shorthan
\newcommand{\sk}{\vspace{.4cm}}
\newcommand{\nochap}{\vspace{0.5cm}}
\newcommand{\nsk}{\vspace{-.4cm}}
\newcommand{\chap}[1]{{\theme \Large \bf #1} \sk}
\newcommand{\R}[1]{{\bl\tt #1}}
\newcommand{\til}{{\footnotesize$\bs{\stackrel{\sim}{}}$ }}

% specific stats markups for this doc
\newcommand{\E}{\ds{E}}
\newcommand{\Reals}{\ds{R}}
\newcommand{\pr}{\text{Pr}}
\newcommand{\var}{\text{var}}
\newcommand{\cov}{\text{cov}}
\newcommand{\mT}{\mc{T}}
\newcommand{\GP}{\mc{GP}}
\newcommand{\iidsim}{\stackrel{\mathrm{iid}}{\sim}}
\newcommand{\indsim}{\stackrel{\mathrm{ind}}{\sim}}
\newcommand{\mN}{\mc{N}}
\newcommand{\mL}{\mc{L}}


\begin{document}

{ \usebackgroundtemplate{}%\includegraphics[height=\paperheight]{phoenix}}
\thispagestyle{empty}
\setcounter{page}{0}

%first review sampling distributions, then talk about sampling distributions for the OLS parameters, then prediction
%right now it goes prediction, review, OLS params, prediction

\title{\theme \Large \vskip 0.5cm
{\bf VectorBiTE Training 2018 \\ Methods Workshop}\\
\bigskip
\bf {\sf \gr Introduction to Bayesian Statistics}}

\author{
\begin{center}
\includegraphics[scale=0.15,trim=10 10 0 150]{VB_logo}
\end{center}
\texttt{\rd\small www.vectorbite.org}
%Normal distribution, distribution of the sample mean, \\
%Central Limit Theorem, Confidence Intervals
%and Method of Moments
%Review of sampling distributions
%Parameter estimation, sampling distributions,\\ 
%coefficient testing, and prediction intervals\\
%\vskip 1cm {\bf Leah R.~Johnson}\\ 
%{\org Virginia Tech}, Statistcs\\
%  \vskip .5cm \texttt{\rd\small leah.johnson-gramacy.com/QED/teaching}
}
\date{}
\maketitle 
}

% doc spacing
\setstretch{1.1}

\begin{frame}
\chap{Recall: Bayes Theorem} 

Bayes Theorem allows us to related the conditional probabilities of two events $A$ and $B$:

\begin{equation*}
\pr(A|B) = \frac{\pr(B|A)\pr(A)}{\pr(B)}
\end{equation*}

The theory of Bayesian statistics is based on {\bl Bayes} theorem.

\end{frame}

\begin{frame}
\chap{What is Bayesian Statistics?} 

Bayesian statistics is an approach for finding information in data and making inference with uncertainty based on the probability distribution of parameters given data.
\sk

This probability distribution is called {\bl posterior} and denoted $f(\theta|Y)$, where

\begin{equation*}
\overbrace{f(\theta|Y)}^\text{Posterior} \propto \overbrace{f(Y|\theta)}^\text{Likelihood} \times \overbrace{f(\theta)}^\text{Prior}
\end{equation*}



\end{frame}

\begin{frame}
\chap{Classical vs Bayesian} 

The fundamental differences between classical and Bayesian methods is what is fixed and what is random in an analysis

\sk
\sk
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Paradigm & Fixed & Random \\
\hline
Classical & param ($\theta$) & data ($Y$)\\
\hline
Bayesian & data ($Y$) & param ($\theta$)\\
\hline
\end{tabular}
\end{center}
\end{frame}


\begin{frame}
\chap{Why/Why Not Bayesian Statistics?} 

{\bf \bl Pros}
\begin{enumerate}
\item We are naturally Bayesian, in terms of being subjective.
\item It provides interpretable answers, such as the 95\% CI.
\item It provides a wide range of possible models, such as hierarchical models and missing data problems.
\end{enumerate}
{\bf \bl Cons}
\begin{enumerate}
\item There is no correct way to choose a prior.
\item It can produce posterior distributions that are heavily influenced by the priors.
\item It often requires high computational cost.
\end{enumerate}


\end{frame}


\begin{frame}
\chap{Steps to Making Inference} 


\begin{enumerate}
\item Research question
\item Data collection
\item Model $Y_i \approx f(X_i)$ 
\item Estimate the parameter in the model with uncertainty
\item Make inference
\end{enumerate}

The difference between {\bl Classical} and {\bl Bayesian} lies in step 4: (C) uses maximum likelihood estimate (MLE), and (B) derives a posterior distribution.
\end{frame}



\begin{frame}
\chap{Example}
\begin{enumerate}
\item {\bf RQ:} whether a cancer drug is effective or not?
\item {\bf Data:}

 \begin{tabular}{lll}
Patient & Effectiveness & Numerical Data \\
1& N& 0\\
2&N&0\\
3&Y&1\\
4&Y&1\\
5&N&0
\end{tabular}
\item {\bf Model:} 

$$Y_i|\rho  \stackrel{iid}{\sim} f(Y|\rho) = \text{Bern}(\rho)$$ 
\end{enumerate}
\end{frame}

\begin{frame}
\begin{enumerate}

\item[4. ] {\bf Posterior:}
First we need to decide on an appropriate  {\bl prior} for $\rho.$ To look at the domain of $\rho$ we recall that 
\begin{equation*}
\rho = \pr(Y_i = 1),\Rightarrow \rho \in [0, 1] 
\end{equation*}
Hence, either a {\bl Beta} or a {\bl Uniform} distribution can be a prior for $\rho.$ Say we chose $\text{Beta}(a,b)$ with $a$ and $b$ specified as $a=b=1$ or $a=1,b=3$. Thus,

$$ f(\rho) = \text{Beta}(a,b).$$
\end{enumerate}
\end{frame}

\begin{frame}
\begin{enumerate}

\item[4. ] {\bf Posterior: continued} 

Next, we need to derive the likelihood. We have $Y_i|\rho   \stackrel{iid}{\sim} f(Y|\rho) = \text{Bern}(\rho),$ where $Y_i|\rho$ are assumed to be {\bl conditionally} independent and identically distributed, thus the likelihood is given by

\begin{align*}
f(\tilde{Y}|\rho) &= f(Y_1, Y_2, \cdots Y_5|\rho)\\
& \stackrel{iid}{=}  f(Y_1|\rho) f(Y_2|\rho)  \cdots f(Y_5|\rho)\\
& = \prod_{i=1}^{n=5} f(Y_i|\rho).
\end{align*}
\end{enumerate}
\end{frame}


\begin{frame}
\begin{enumerate}

\item[4. ] {\bf Posterior: continued} 

Now we can use {\bl Bayes} formula to calculate the posterior distribution as follows
\begin{align*}
f(\rho|\tilde{Y}) &\propto \prod_{i=1}^{n=5} f(Y_i|\rho) f(\rho)\\
&\propto  \prod_{i=1}^{n=5} \text{Bern}(Y_i|\rho)\text{Beta}(a,b)\\
&\propto  \prod_{i=1}^{n=5} \rho^{Y_i} (1- \rho)^{n-Y_i} \dfrac{\Gamma (a+b)}{\Gamma (a)\Gamma (b)} \rho^{a-1} (1-\rho)^{b-1}
\end{align*}

Does this look like a known distribution? What do we do?

\end{enumerate}
\end{frame}


\begin{frame}
\begin{enumerate}

\item[4. ] {\bf Posterior: continued} 

We play the game {\bl name that distribution}
\begin{enumerate}
\item[a.] Simplify the best you can
\item[b.] $\ast$ , $\div$ by constants
\item[c.] Remember what is the RV in the expression
\item[d.] Look for {\bl Kernels} of known distributions
\end{enumerate}
\sk
{\rd Definition:} The {\bl Kernel} of a probability density function (or mass function) are all components which include the random variable. 

eg: Let $Y_i \sim \textbf{N}(\mu, \sigma^2),$ with random variable $\mu$

$$ f(Y_i) = \underbrace{\dfrac{1}{\sqrt{2\pi \sigma^2}}}_{constant} \underbrace{e^{-\dfrac{1}{2\sigma^2}(Y_i - \mu)^2}}_{Kernel}$$ 
\end{enumerate}
\end{frame}


\begin{frame}
\begin{enumerate}

\item[4. ] {\bf Posterior: continued} 

Back to our example: 
\begin{align*}
f(\rho|\tilde{Y}) &\propto  \prod_{i=1}^{n=5} \rho^{Y_i} (1- \rho)^{n-Y_i} \dfrac{\Gamma (a+b)}{\Gamma (a)\Gamma (b)} \rho^{a-1} (1-\rho)^{b-1}\\
&\propto \rho^{\sum Y_i} (1- \rho)^{n-\sum Y_i} \dfrac{\Gamma (a+b)}{\Gamma (a)\Gamma (b)} \rho^{a-1} (1-\rho)^{b-1}\\
&\propto \rho^{\sum Y_i +a -1} (1- \rho)^{n-\sum Y_i +b -1} \\
&\propto \textbf{Beta} \left(\underbrace{\sum Y_i +a}_{a^*}, \underbrace{n-\sum Y_i +b}_{b^*}\right),
\end{align*}
where
$ \textbf{Beta}(a^*, b^*) = \dfrac{\Gamma (a^*+b^*)}{\Gamma (a^*)\Gamma (b^*)} \rho^{a^*-1} (1-\rho)^{b^*-1}.$
{\rd Now what do we do with this posterior?}

\end{enumerate}
\end{frame}

\begin{frame}
\begin{enumerate}

\item[5.] {\bf Make inference:} We summarize the posterior to make inference using the following statistics
\begin{enumerate}
\item[a.] Expectation $\text{E}(\rho|\tilde{Y})$% = \dfrac{a^*}{a^* +b^*} = 0.25$
\item[b.] Maximum A Posteriori (MAP) $\text(Mode) (f(\rho|\tilde{Y})$% = = \dfrac{a^* -1}{a^* +b^* -2} = 0.2$
\item[c.] Variance $\text{Var}(\rho|\tilde{Y} )$%= (.12)^2$
\item[d.] Calculate $\pr(\rho<.5|\tilde{Y}) $%= .96$
\item[e.] Q Credible Interval $\pr(\rho \in [L, U] |\tilde{Y}) = Q,$ with $L = \dfrac{1-Q}{2},$ and $U = \dfrac{1-Q}{2} +Q$
\item[f.] Q Highest Posterior Density Interval (HPDI) $\pr(\rho \in [L, U] |\tilde{Y}) = Q,$ where $L$ and $U$ are chosen so that $U-L$ is the shortest
\end{enumerate}
 \end{enumerate}
\end{frame}


\begin{frame}
\begin{enumerate}

\item[5.] {\bf Inference Statements} 

{\bf For a, b, c:} Given the data, we predict that the probability that the cancer drug is effective is $\text{E}(\rho|\tilde{Y})$ or  $\text{MAP}(\rho|\tilde{Y})$ with variance $\text{Var}(\rho|\tilde{Y})$. \\

{\bf For d:} Given the data, we infer the prob of the cancer drug being effective is low because $\pr(\rho<.5|\tilde{Y}) = .96$ or high because $\pr(\rho<.5|\tilde{Y}) = .2$\\

{\bf For e, f:} Given the data, the probability that $\rho \in [L, U]$ is Q. \\

{\rd \bf Note:} {\bf QCI} Describes the behavior of the {\bl  tail} of the posterior distribution and { \bf HPDI} Describes where the {\bl mean} of the posterior distribution is.\\


\end{enumerate}
\end{frame}


\begin{frame}
\chap{Bayesian Models:}

\begin{tabular}{ll}
Beta-Bernoulli & Gamma-Poisson\\
$Y_i|\rho \sim \textbf{Bern}(\rho)$ & $Y_i|\lambda \sim \textbf{Pois}(\rho)$ \\
$\rho \sim \textbf{Beta}(a,b)$ &  $\lambda \sim \textbf{Gamma}(a,b)$\\
$\rho|\tilde{Y} \sim \textbf{Beta}(a^*, b^*)$ & $\lambda|\tilde{Y} \sim \textbf{Gamma}(a^*, b^*)$\\
\end{tabular}
\sk

{\rd \bf Question:} Are all posteriors in Bayesian analysis the same family as the priors?

{\rd \bf Answer:} No 

\end{frame}

\begin{frame}
\chap{Definition of Conjugacy:}

Consider a parameter $\theta,$ if the derived posterior distribution $f(\theta|\tilde{Y})$ has the same form as the prior $f(\theta),$ then the prior is called {\bl conjugate prior distribution} 


\end{frame}

\begin{frame}
\chap{Examples of Conjugate Priors}
\nsk


\end{frame}




\end{document}